{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans = THE CLEAREST NEURAL NETWORK FRAMEWORK BY UNDWAD\n",
      "ans = TIC-TAC-TOE GAME\n",
      "winner =  1\n",
      "s =\n",
      "\n",
      "   2   0   1\n",
      "   1   2   1\n",
      "   2   0   1\n",
      "\n",
      "oh18 =\n",
      "\n",
      "   0   1   1   0   0   1   0   0   0   1   0   0   1   0   1   0   1   0\n",
      "\n",
      "oh27 =\n",
      "\n",
      " Columns 1 through 26:\n",
      "\n",
      "  0  0  0  1  0  1  0  0  0  0  1  0  0  0  0  1  1  1  1  0  1  0  1  0  0  0\n",
      "\n",
      " Column 27:\n",
      "\n",
      "  0\n",
      "\n",
      "playing 100 times randompolicy vs randompolicy\n",
      "wins = [58 23]\n",
      "draws = 19\n"
     ]
    }
   ],
   "source": [
    "clear all;\n",
    "\n",
    "global ipynb = 'ttt';\n",
    "\n",
    "source('clearest-nn.m');\n",
    "source('utils-logging.m');\n",
    "source('utils-training.m');\n",
    "source('game-ttt.m');\n",
    "\n",
    "% log2file(tmp('log'));\n",
    "\n",
    "##########################################\n",
    "\n",
    "[winner,s] = play1(@randompolicy, @randompolicy)\n",
    "oh18 = game2oh18(s)'\n",
    "oh27 = game2oh27(s)'\n",
    "play(100, @randompolicy);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path = tmp/tic-tac-toe.dataset.mat\n",
      "exists = 1\n",
      "num_s = 19683\n",
      "player(states(:,:,100)) = 1\n",
      "turns(100) = 1\n",
      "states(:,:,100) = [0 0 0;0 1 0;2 0 0]\n",
      "V(100) = 0\n",
      "pi(:,100) = [0.109167230004231;0.160779337725954;0;0.136495405475451;0;0.138484815816882;0.148083386090299;0.189783691910609;0.117206131078737]\n",
      "sum(pi(:,100)) = 1.000000\n",
      "player(states(:,:,101)) = 2\n",
      "turns(101) = 2\n",
      "states(:,:,101) = [1 0 0;0 1 0;2 0 0]\n",
      "V(101) = 0\n",
      "pi(:,101) = [0;0.154605399819868;0;0.122024599336157;0;0.272938546704486;0.134159348916976;0.139850816077742;0.176421286415385]\n",
      "sum(pi(:,101)) = 1.000000\n",
      "num_game_s = 6046\n"
     ]
    }
   ],
   "source": [
    "### MODEL ###\n",
    "\n",
    "global states turns V pi Q;\n",
    "\n",
    "function enum(s = game(), t = tic())\n",
    "    global states turns V pi;\n",
    "    idx = state2num(s);\n",
    "    showlog(1, 50, '%d %s %s', idx, mat2str(s))\n",
    "    if turns(idx) == 0\n",
    "        states(:,:,idx) = s;\n",
    "        turns(idx)      = player(s);\n",
    "        aaa             = actions(s);\n",
    "        na              = count(aaa);\n",
    "        if na > 0\n",
    "            pi(aaa,idx) = softmax(rand(na,1));\n",
    "        end\n",
    "        for a = aaa\n",
    "            enum(game(s, a), t);\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "rand('state', 1);\n",
    "\n",
    "path   = tmp('dataset.mat')\n",
    "exists = logical(exist(path))\n",
    "if exists\n",
    "    load('-binary', path, 'states', 'turns', 'V', 'pi', 'Q');\n",
    "else\n",
    "    n      = 3^9;\n",
    "    states = zeros(3,3,n);\n",
    "    turns  = zeros(1,n);\n",
    "    V      = zeros(1,n);\n",
    "    pi     = zeros(9,n);\n",
    "    Q      = zeros(9,n);\n",
    "    printstart();\n",
    "    enum();\n",
    "    Q = pi;\n",
    "    save('-binary', path, 'states', 'turns', 'V', 'pi', 'Q');\n",
    "    printend(path);    \n",
    "end\n",
    "\n",
    "num_s = count(states);\n",
    "printvar('num_s');\n",
    "printvar('player(states(:,:,100))');\n",
    "printvar('turns(100)');\n",
    "printvar('states(:,:,100)');\n",
    "printvar('V(100)');\n",
    "printvar('pi(:,100)');\n",
    "printvar('sum(pi(:,100))');\n",
    "printvar('player(states(:,:,101))');\n",
    "printvar('turns(101)');\n",
    "printvar('states(:,:,101)');\n",
    "printvar('V(101)');\n",
    "printvar('pi(:,101)');\n",
    "printvar('sum(pi(:,101))');\n",
    "\n",
    "num_game_s = 0;\n",
    "for i=1:num_s\n",
    "    if turns(i) != 0\n",
    "        s = states(:,:,i);\n",
    "        p = player(s);\n",
    "        assert(p == turns(p));\n",
    "        s_ = num2state(i);\n",
    "        assert(all(all(s_ == s)));\n",
    "        num_game_s += 1;\n",
    "    end\n",
    "end\n",
    "\n",
    "printvar('num_game_s');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s =\n",
      "\n",
      "   1   1   0\n",
      "   2   2   1\n",
      "   2   1   0\n",
      "\n",
      "p =  2\n",
      "aaa =\n",
      "\n",
      "   7   9\n",
      "\n",
      "a =  9\n",
      "next =\n",
      "\n",
      "   3\n",
      "  -1\n",
      "\n",
      "playing 100 times randompolicy vs pipolicy\n",
      "wins = [66 25]\n",
      "draws = 9\n",
      "playing 100 times pipolicy vs randompolicy\n",
      "wins = [65 25]\n",
      "draws = 10\n"
     ]
    }
   ],
   "source": [
    "### GAME DYNAMICS - p(s',r|s,a) ###\n",
    "\n",
    "function a = pipolicy(s)\n",
    "    global pi;\n",
    "    idx   = state2num(s);\n",
    "    [~,a] = max(pi(:,idx));\n",
    "end\n",
    "\n",
    "# next        = [ [next_idx; reward], ... ]\n",
    "# probability = 1/count(next)\n",
    "function next = dynamics(s, a) \n",
    "    # 3 = 2 + 1 = smallest terminal state index \n",
    "    global states;\n",
    "    s = game(s,a);\n",
    "    if iswin(s,a)  \n",
    "        next = [ 3; 1 ]; # learning player wins\n",
    "    elseif isover(s) \n",
    "        next = [ 3; 0 ]; # draw\n",
    "    else\n",
    "        aaa  = actions(s);\n",
    "        na   = count(aaa);\n",
    "        next = zeros(2,na);\n",
    "        for i = 1:na\n",
    "            a   = aaa(i);\n",
    "            s_  = game(s,a);\n",
    "            idx = state2num(s_);\n",
    "            if iswin(s_,a)   \n",
    "                next(:,i) = [ 3;  -1 ]; # other player wins\n",
    "            elseif isover(s_)\n",
    "                next(:,i) = [ 3;   0 ]; # draw\n",
    "            else\n",
    "                next(:,i) = [ idx; 0 ]; # continue playing\n",
    "            end  \n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "do \n",
    "    s = pick(states);\n",
    "until nnz(s) > 0;\n",
    "s\n",
    "p    = player(s)\n",
    "aaa  = actions(s)\n",
    "a    = pick(aaa)\n",
    "next = dynamics(s, a)\n",
    "\n",
    "play(100, @randompolicy, @pipolicy);\n",
    "play(100, @pipolicy, @randompolicy);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"assets/dp-policy-iter.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path = tmp/tic-tac-toe.dynamic-prog-policy-iter.mat\n",
      "exists = 1\n",
      "playing 100 times pipolicy vs randompolicy\n",
      "wins = [100 0]\n",
      "draws = 0\n",
      "playing 100 times randompolicy vs pipolicy\n",
      "wins = [1 94]\n",
      "draws = 5\n",
      "playing 100 times pipolicy vs pipolicy\n",
      "wins = [0 0]\n",
      "draws = 100\n"
     ]
    }
   ],
   "source": [
    "### DYNAMIC PROGRAMMING POLICY ITERATION ###\n",
    "\n",
    "global theta gamma;\n",
    "\n",
    "function vvv = bellman_values(s, a)\n",
    "    global V gamma;\n",
    "    next   = dynamics(s, a);\n",
    "    n      = count(next);\n",
    "    vvv    = zeros(1,n);\n",
    "    for i  = 1:n \n",
    "        [next_idx, reward] = split(next(:, i));\n",
    "        vvv(i)             = 1/n * (reward + gamma*V(next_idx));\n",
    "    end\n",
    "end\n",
    "\n",
    "function bellman_update(idx)\n",
    "    global states V pi;\n",
    "    s     = states(:,:,idx);\n",
    "    ppp   = pi(:,idx);\n",
    "    v     = 0;\n",
    "    for a = actions(s) \n",
    "        vvv = ppp(a) .* bellman_values(s, a);\n",
    "        v  += sum(vvv);  \n",
    "    end\n",
    "    V(idx) = v;\n",
    "end\n",
    "\n",
    "function greedify_policy(idx)\n",
    "    global states V pi;\n",
    "    s      = states(:,:,idx);\n",
    "    max_qa = 0;\n",
    "    best_a = 0;\n",
    "    for a = actions(s) \n",
    "        vvv = bellman_values(s, a);\n",
    "        qa  = sum(vvv); \n",
    "        if qa > max_qa\n",
    "            max_qa = qa;\n",
    "            best_a = a;\n",
    "        end\n",
    "    end\n",
    "    if best_a > 0\n",
    "        pi(:,idx)      = 0;\n",
    "        pi(best_a,idx) = 1;\n",
    "    end\n",
    "end\n",
    "\n",
    "function evaluate_policy(p)\n",
    "    global states turns V theta;\n",
    "    do # global loop until delta < theta\n",
    "        delta = 0;\n",
    "        num_s = count(states);\n",
    "        for idx = 1:num_s # iter all states\n",
    "            if turns(idx) == p # learning player's state\n",
    "                v = V(idx);\n",
    "                bellman_update(idx);\n",
    "                delta = max(delta, abs(v - V(idx)));\n",
    "                showlog(1, 50, '%s %d %d %s %f', 'evaluate_policy', p, idx, dec2base(idx-1,3), delta);\n",
    "            end\n",
    "        end\n",
    "    until delta < theta;\n",
    "end\n",
    "\n",
    "function stable = improve_policy(p)\n",
    "    global states turns pi;\n",
    "    stable = true;\n",
    "    num_s = count(states);\n",
    "    for idx = 1:num_s # iter all states\n",
    "        if turns(idx) == p # learning player's state\n",
    "            ppp = pi(:,idx);\n",
    "            greedify_policy(idx);\n",
    "            if any(ppp != pi(:,idx))\n",
    "                stable = false;\n",
    "            end\n",
    "            showlog(1, 50, '%s %d %d %s %s', 'improve_policy', p, idx, dec2base(idx-1,3), bool2yesno(stable));\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function policy_iteration(p)\n",
    "    do\n",
    "        evaluate_policy(p);\n",
    "        stable = improve_policy(p);\n",
    "    until stable;\n",
    "end\n",
    "\n",
    "theta = 0.0001;\n",
    "gamma = 0.5;\n",
    "\n",
    "path   = tmp('dynamic-prog-policy-iter.mat')\n",
    "exists = logical(exist(path))\n",
    "if exists\n",
    "    load('-binary', path, 'V', 'pi');\n",
    "else\n",
    "    printstart();\n",
    "    policy_iteration(1);\n",
    "    policy_iteration(2);\n",
    "    save('-binary', path, 'V', 'pi');\n",
    "    printend(path);    \n",
    "end\n",
    "\n",
    "play(100, @pipolicy, @randompolicy);\n",
    "play(100, @randompolicy, @pipolicy);\n",
    "play(100, @pipolicy);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"assets/dp-value-iter.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path = tmp/tic-tac-toe.dynamic-prog-value-iter.mat\n",
      "exists = 1\n",
      "playing 100 times pipolicy vs randompolicy\n",
      "wins = [100 0]\n",
      "draws = 0\n",
      "playing 100 times randompolicy vs pipolicy\n",
      "wins = [2 96]\n",
      "draws = 2\n",
      "playing 100 times pipolicy vs pipolicy\n",
      "wins = [0 100]\n",
      "draws = 0\n"
     ]
    }
   ],
   "source": [
    "### DYNAMIC PROGRAMMING VALUE ITERATION ###\n",
    "\n",
    "function bellman_update(idx)\n",
    "    global states V pi;\n",
    "    s     = states(:,:,idx);\n",
    "    ppp   = pi(:,idx);\n",
    "    max_v = 0;\n",
    "    for a = actions(s) \n",
    "        vvv   = ppp(a) .* bellman_values(s, a);\n",
    "        max_v = max(sum(vvv), max_v);\n",
    "    end\n",
    "    V(idx) = max_v;\n",
    "end\n",
    "\n",
    "theta = 0.0001;\n",
    "gamma = 0.5;\n",
    "\n",
    "path   = tmp('dynamic-prog-value-iter.mat')\n",
    "exists = logical(exist(path))\n",
    "if exists\n",
    "    load('-binary', path, 'V', 'pi');\n",
    "else\n",
    "    printstart();\n",
    "    policy_iteration(1);\n",
    "    policy_iteration(2);\n",
    "    save('-binary', path, 'V', 'pi');\n",
    "    printend(path);    \n",
    "end\n",
    "\n",
    "play(100, @pipolicy, @randompolicy);\n",
    "play(100, @randompolicy, @pipolicy);\n",
    "play(100, @pipolicy);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"assets/sarsa-iter.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path = tmp/tic-tac-toe.sarsa.mat\n",
      "exists = 1\n",
      "playing 100 times qpolicy vs randompolicy\n",
      "wins = [97 1]\n",
      "draws = 2\n",
      "playing 100 times randompolicy vs qpolicy\n",
      "wins = [3 89]\n",
      "draws = 8\n",
      "playing 100 times qpolicy vs qpolicy\n",
      "wins = [0 0]\n",
      "draws = 100\n"
     ]
    }
   ],
   "source": [
    "### SARSA (ON-POLICY TEMPORAL DIFFERENCE CONTROL) ###\n",
    "\n",
    "global epsilon alpha gamma Q;\n",
    "\n",
    "function [a,q] = qpolicy(s)\n",
    "    global Q;\n",
    "    aaa   = actions(s);\n",
    "    idx   = state2num(s);\n",
    "    qqq   = Q(:,idx)(aaa);\n",
    "    [q,i] = max(qqq);\n",
    "    a     = aaa(i);\n",
    "end\n",
    "\n",
    "function a = epsilon_greedy_qpolicy(s)\n",
    "    global epsilon;\n",
    "    if rand() < epsilon\n",
    "        a = pick(actions(s));\n",
    "    else\n",
    "        a = qpolicy(s);\n",
    "    end\n",
    "end\n",
    "\n",
    "function winner = sarsa_episode(target, p)\n",
    "    global alpha gamma Q;\n",
    "    winner = [];\n",
    "    if p == 1\n",
    "        s = game();\n",
    "        a = epsilon_greedy_qpolicy(s);\n",
    "    elseif p == 2\n",
    "        s = game();\n",
    "        a = randompolicy(s);\n",
    "        s = game(s, a);\n",
    "        a = epsilon_greedy_qpolicy(s);        \n",
    "    end\n",
    "    do\n",
    "        idx    = state2num(s);\n",
    "        q      = Q(a, idx);\n",
    "        next_s = game(s, a);               # learning player moves\n",
    "        if iswin(next_s, a)                # learning player wins\n",
    "            reward = 1;\n",
    "            next_q = 0;\n",
    "            winner = p;\n",
    "        elseif isover(next_s)              # draw\n",
    "            reward = 0;\n",
    "            next_q = 0;\n",
    "            winner = 0;\n",
    "        else                               # continue\n",
    "            oppo_a = randompolicy(next_s); \n",
    "            next_s = game(next_s, oppo_a); # other player moves\n",
    "            if iswin(next_s, oppo_a)       # other player wins\n",
    "                reward = -1;\n",
    "                next_q = 0;\n",
    "                winner = player(next_s, oppo_a);\n",
    "            elseif isover(next_s)          # draw\n",
    "                reward = 0;\n",
    "                next_q = 0;\n",
    "                winner = 0;\n",
    "            else                           # continue\n",
    "                reward = 0;\n",
    "                [next_a,next_q] = target(next_s);\n",
    "            end  \n",
    "        end  \n",
    "        Q(a, idx) = q + alpha*(reward + gamma*next_q - q);\n",
    "        s = next_s;\n",
    "        a = next_a;\n",
    "    until !isempty(winner);    \n",
    "end\n",
    "\n",
    "function [a,q] = sarsa_epsilon_greedy_target(s)\n",
    "    global Q;\n",
    "    a = epsilon_greedy_qpolicy(s);\n",
    "    q = Q(a, state2num(s));\n",
    "end\n",
    "\n",
    "function sarsa_iteration(target, p, n)\n",
    "    wins = zeros(3,1);\n",
    "    for i = 1:n\n",
    "        showlog(1, 80, 'sarsa_iteration for %d, %d of %d, %s', p, i, n, mat2str(wins));\n",
    "        winner = sarsa_episode(target, p);\n",
    "        wins(winner+1) += 1;\n",
    "    end\n",
    "end\n",
    "\n",
    "theta   = 0.0001;\n",
    "epsilon = 0.1;\n",
    "alpha   = 0.1;\n",
    "gamma   = 0.5;\n",
    "iters   = 10000;\n",
    "\n",
    "path   = tmp('sarsa.mat')\n",
    "exists = logical(exist(path))\n",
    "if exists\n",
    "    load('-binary', path, 'Q');\n",
    "else\n",
    "    printstart();\n",
    "    sarsa_iteration(@sarsa_epsilon_greedy_target, 1, iters);\n",
    "    sarsa_iteration(@sarsa_epsilon_greedy_target, 2, iters);\n",
    "    save('-binary', path, 'Q');\n",
    "    printend(path);    \n",
    "end\n",
    "\n",
    "play(100, @qpolicy, @randompolicy);\n",
    "play(100, @randompolicy, @qpolicy);\n",
    "play(100, @qpolicy);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"assets/q-learning-iter.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path = tmp/tic-tac-toe.sarsa-max.mat\n",
      "exists = 1\n",
      "playing 100 times qpolicy vs randompolicy\n",
      "wins = [100 0]\n",
      "draws = 0\n",
      "playing 100 times randompolicy vs qpolicy\n",
      "wins = [2 90]\n",
      "draws = 8\n",
      "playing 100 times qpolicy vs qpolicy\n",
      "wins = [0 0]\n",
      "draws = 100\n"
     ]
    }
   ],
   "source": [
    "### Q-LEARNING \\ SARSAMAX (OFF-POLICY TEMPORAL DIFFERENCE CONTROL) ###\n",
    "\n",
    "function [a,q] = sarsa_max_target(s)\n",
    "    global Q;\n",
    "    aaa   = actions(s);\n",
    "    qqq   = Q(:, state2num(s));\n",
    "    q     = max(qqq(aaa));\n",
    "    a     = epsilon_greedy_qpolicy(s);\n",
    "end\n",
    "\n",
    "theta   = 0.0001;\n",
    "epsilon = 0.1;\n",
    "alpha   = 0.1;\n",
    "gamma   = 0.5;\n",
    "iters   = 10000;\n",
    "\n",
    "path   = tmp('sarsa-max.mat')\n",
    "exists = logical(exist(path))\n",
    "if exists\n",
    "    load('-binary', path, 'Q');\n",
    "else\n",
    "    printstart();\n",
    "    sarsa_iteration(@sarsa_max_target, 1, iters);\n",
    "    sarsa_iteration(@sarsa_max_target, 2, iters);\n",
    "    save('-binary', path, 'Q');\n",
    "    printend(path);    \n",
    "end\n",
    "\n",
    "play(100, @qpolicy, @randompolicy);\n",
    "play(100, @randompolicy, @qpolicy);\n",
    "play(100, @qpolicy);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path = tmp/tic-tac-toe.sarsa-mean.mat\n",
      "exists = 1\n",
      "playing 100 times qpolicy vs randompolicy\n",
      "wins = [94 3]\n",
      "draws = 3\n",
      "playing 100 times randompolicy vs qpolicy\n",
      "wins = [1 87]\n",
      "draws = 12\n",
      "playing 100 times qpolicy vs qpolicy\n",
      "wins = [0 0]\n",
      "draws = 100\n"
     ]
    }
   ],
   "source": [
    "### EXPECTED SARSA \\ SARSAMEAN (OFF-POLICY TEMPORAL DIFFERENCE CONTROL) ###\n",
    "\n",
    "function [a,q] = sarsa_mean_target(s)\n",
    "    global Q;\n",
    "    aaa   = actions(s);\n",
    "    qqq   = Q(:, state2num(s));\n",
    "    q     = mean(qqq(aaa));\n",
    "    a     = epsilon_greedy_qpolicy(s);\n",
    "end\n",
    "\n",
    "theta   = 0.0001;\n",
    "epsilon = 0.1;\n",
    "alpha   = 0.1;\n",
    "gamma   = 0.5;\n",
    "iters   = 10000;\n",
    "\n",
    "path   = tmp('sarsa-mean.mat')\n",
    "exists = logical(exist(path))\n",
    "if exists\n",
    "    load('-binary', path, 'Q');\n",
    "else\n",
    "    printstart();\n",
    "    sarsa_iteration(@sarsa_mean_target, 1, iters);\n",
    "    sarsa_iteration(@sarsa_mean_target, 2, iters);\n",
    "    save('-binary', path, 'Q');\n",
    "    printend(path);    \n",
    "end\n",
    "\n",
    "play(100, @qpolicy, @randompolicy);\n",
    "play(100, @randompolicy, @qpolicy);\n",
    "play(100, @qpolicy);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"assets/td-advantage-actor-critic.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winner =  1\n",
      "s =\n",
      "\n",
      "   1   1   1\n",
      "   2   1   2\n",
      "   2   1   2\n",
      "\n",
      "playing 100 times randompolicy vs actorpolicy\n",
      "wins = [52 38]\n",
      "draws = 10\n",
      "path = tmp/tic-tac-toe.actor-critic.mat\n",
      "exists = 1\n",
      "playing 100 times randompolicy vs actorpolicy\n",
      "wins = [5 85]\n",
      "draws = 10\n"
     ]
    }
   ],
   "source": [
    "### TEMPORAL DIFFERENCE ADVANTAGE ACTOR-CRITIC ###\n",
    "\n",
    "% rand('state', 1);\n",
    "\n",
    "global actor critic gamma;\n",
    "\n",
    "function delta = criticize(prev_s, next_s, reward, winner)\n",
    "    global critic gamma;\n",
    "    if isempty(winner)\n",
    "        x            = game2onehot(next_s);\n",
    "        [~,y]        = forward(critic, x);\n",
    "    else\n",
    "        y            = 0;\n",
    "    end\n",
    "    y                = reward + gamma*y;\n",
    "    x                = game2onehot(prev_s);\n",
    "    [critic,z]       = forward(critic, x);\n",
    "    dE               = gradient(critic, z, y);\n",
    "    [critic, ggg, ~] = backward(critic, z, dE);\n",
    "    [critic, ggg]    = optimize_gradient(critic, ggg, 1);\n",
    "    critic           = update(critic, ggg);\n",
    "    delta            = y - z; \n",
    "end\n",
    "\n",
    "function [a,z] = actorpolicy(s)\n",
    "    global actor;\n",
    "    x         = game2onehot(s);\n",
    "    [actor,z] = forward(actor, x);\n",
    "    aaa       = actions(s); \n",
    "    z_        = zeros(9,1);\n",
    "    x         = actor.XXX{end}(aaa);\n",
    "    z_(aaa)   = softmax(x);\n",
    "    f         = mnrnd(1,z_);\n",
    "    a         = find(f); \n",
    "end\n",
    "\n",
    "function learn2act(a, z, delta, eps=1e-8)\n",
    "    global actor;\n",
    "    y               = zeros(size(z));\n",
    "    y(a)            = delta;\n",
    "    dE              = gradient(actor, z, y);\n",
    "    [actor, ggg, ~] = backward(actor, z, dE);\n",
    "    [actor, ggg]    = optimize_gradient(actor, ggg, 1);\n",
    "    actor           = update(actor, -ggg);\n",
    "end\n",
    "\n",
    "function winner = episode(p)\n",
    "    global actor critic gamma;\n",
    "    winner = [];\n",
    "    s      = game();\n",
    "    if p == 2\n",
    "        a = randompolicy(s);\n",
    "        s = game(s, a); \n",
    "    end\n",
    "    do\n",
    "        prev_s = s;\n",
    "        [a,z]  = actorpolicy(s); \n",
    "        s      = game(s, a);        # learning player moves\n",
    "        if iswin(s, a)                 # learning player wins\n",
    "            reward = 1;\n",
    "            winner = p;\n",
    "        elseif isover(s)               # draw\n",
    "            reward = 0;\n",
    "            winner = 0;\n",
    "        elseif player(s) == p          # invalid move\n",
    "            error('invalid move');\n",
    "        else                           # continue\n",
    "            a = randompolicy(s); \n",
    "            s = game(s, a);            # other player moves\n",
    "            if iswin(s, a)             # other player wins\n",
    "                reward = -1;\n",
    "                winner = player(s, a);\n",
    "            elseif isover(s)           # draw\n",
    "                reward = 0;\n",
    "                winner = 0;\n",
    "            else                       # continue\n",
    "                reward = 0;\n",
    "            end  \n",
    "        end  \n",
    "        delta = criticize(prev_s, s, reward, winner);\n",
    "        learn2act(a, z, delta);\n",
    "    until !isempty(winner);    \n",
    "end\n",
    "\n",
    "function ratio = iteration(p, n, eps=1e-8)\n",
    "    global critic actor aEEE cEEE;\n",
    "    RATIO = zeros(1,n);\n",
    "    wins  = zeros(3,1);\n",
    "    for i = 1:n\n",
    "        winner          = episode(p);\n",
    "        wins(winner+1) += 1;\n",
    "        ratio           = wins(1+1)/wins(2+1);\n",
    "        RATIO(i)        = ratio;\n",
    "        critic_gradnorm = getunit(critic.optimizers,'gradient_clipping').norm;\n",
    "        actor_gradnorm  = getunit( actor.optimizers,'gradient_clipping').norm;\n",
    "        critic_updratio = getunit(critic.optimizers,'stats').ratio;\n",
    "        actor_updratio  = getunit( actor.optimizers,'stats').ratio;\n",
    "        showlog(1, 80, 'player %d episode %d of %d, gradnorm %f vs %f, updratio %f vs %f, wins %s, %f', \n",
    "                        p, i, n, critic_gradnorm, actor_gradnorm, critic_updratio, actor_updratio, mat2str(wins), ratio);\n",
    "    end\n",
    "    figure('Position', [0 0 1000 400]);\n",
    "    plot(1:n, RATIO, 'b');\n",
    "    title('win ratio history');\n",
    "end\n",
    "\n",
    "gamma  = 1;\n",
    "critic = model(18, {'dense', 1});\n",
    "critic = optimization(critic, {'adam', 0.1},   {'gradient_clipping', 0.9}, 'stats');\n",
    "critic = objective(critic, 'mse');\n",
    "actor  = model(18, {'dense', 9}, 'softmax');\n",
    "actor  = optimization(actor,  {'adam', 0.03},  {'gradient_clipping', 0.9}, 'stats');\n",
    "actor  = objective(actor, 'logloss');\n",
    "\n",
    "% printmodel('critic');\n",
    "% printmodel('actor');\n",
    "\n",
    "[winner,s] = play1(@randompolicy, @actorpolicy)\n",
    "play(100, @randompolicy, @actorpolicy);\n",
    "\n",
    "path   = tmp('actor-critic.mat')\n",
    "exists = logical(exist(path))\n",
    "if exists\n",
    "    load('-binary', path, 'actor', 'critic');\n",
    "else\n",
    "    printstart();\n",
    "    ratio = iteration(2, 1000);\n",
    "    save('-binary', path, 'actor', 'critic');\n",
    "    printend(sprintf('%s %f', path, ratio));    \n",
    "end\n",
    "\n",
    "play(100, @randompolicy, @actorpolicy);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing 1000 times randompolicy vs randompolicy\n",
      "wins = [579 291]\n",
      "draws = 130\n",
      "playing 1000 times randompolicy vs pipolicy\n",
      "wins = [41 927]\n",
      "draws = 32\n",
      "playing 1000 times randompolicy vs qpolicy\n",
      "wins = [28 852]\n",
      "draws = 120\n",
      "playing 1000 times randompolicy vs actorpolicy\n",
      "wins = [124 782]\n",
      "draws = 94\n",
      "playing 1000 times pipolicy vs actorpolicy\n",
      "wins = [234 764]\n",
      "draws = 2\n",
      "playing 1000 times qpolicy vs actorpolicy\n",
      "wins = [356 0]\n",
      "draws = 644\n",
      "playing 1000 times pipolicy vs qpolicy\n",
      "wins = [1000 0]\n",
      "draws = 0\n",
      "playing 1000 times qpolicy vs pipolicy\n",
      "wins = [0 0]\n",
      "draws = 1000\n"
     ]
    }
   ],
   "source": [
    "play(1000, @randompolicy, @randompolicy);\n",
    "play(1000, @randompolicy, @pipolicy);\n",
    "play(1000, @randompolicy, @qpolicy);\n",
    "play(1000, @randompolicy, @actorpolicy);\n",
    "play(1000, @pipolicy,     @actorpolicy);\n",
    "play(1000, @qpolicy,      @actorpolicy);\n",
    "play(1000, @pipolicy,     @qpolicy);\n",
    "play(1000, @qpolicy,      @pipolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
