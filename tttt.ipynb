{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans = THE CLEAREST NEURAL NETWORK FRAMEWORK BY UNDWAD\n",
      "ans = TIC-TAC-TOE TACTICS GAME\n"
     ]
    }
   ],
   "source": [
    "clear all;\n",
    "\n",
    "global ipynb = 'tttt';\n",
    "\n",
    "source('clearest-nn.m');\n",
    "source('utils-logging.m');\n",
    "source('utils-training.m');\n",
    "source('game-tttt.m');\n",
    "\n",
    "log2file(tmp('log'));\n",
    "\n",
    "% rand('state', 1);\n",
    "\n",
    "##########################################\n",
    "\n",
    "% [winner,s] = play1(@randompolicy, @randompolicy);\n",
    "% winner\n",
    "% size(game2oh18(s)')\n",
    "% size(game2oh27(s)')\n",
    "% s    = game2mat(s)\n",
    "% play(100, @randompolicy);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vmodel.num_p = 84501\n",
      "playing 100 times randompolicy vs randompolicy\n",
      "game 100, winner 0, wins [44 34], draws 22                                      \n",
      "playing 100 times randompolicy vs stochasticpolicy\n",
      "game 13, winner 2, wins [5 6], draws 2                                          "
     ]
    }
   ],
   "source": [
    "global vmodel epsilon tau gamma agentpolicy rivalpolicy;\n",
    "\n",
    "function X = nextstates2onehots(s, aaa=actions(s))\n",
    "    X = map(@(a) game2oh18(game(s,a)), aaa);\n",
    "end\n",
    "\n",
    "function a = stochasticpolicy(s)\n",
    "    global vmodel;\n",
    "    aaa     = actions(s);\n",
    "    X       = nextstates2onehots(s, aaa);\n",
    "    [~,Q]   = forward(vmodel, X);\n",
    "    [~,~,i] = softmaxpick(Q');\n",
    "    a       = aaa(i);\n",
    "end\n",
    "\n",
    "function a = deterministicpolicy(s)\n",
    "    global vmodel;\n",
    "    aaa   = actions(s);\n",
    "    X     = nextstates2onehots(s, aaa);\n",
    "    [~,Q] = forward(vmodel, X);\n",
    "    [~,i] = max(Q);\n",
    "    a     = aaa(i);\n",
    "end\n",
    "\n",
    "function [a,x,p] = taustochasticpolicy(s)\n",
    "    global vmodel tau;\n",
    "    aaa     = actions(s);\n",
    "    X       = nextstates2onehots(s, aaa);\n",
    "    [~,Q]   = forward(vmodel, X);\n",
    "    Q      /= tau;\n",
    "    [~,p,i] = softmaxpick(Q');\n",
    "    a       = aaa(i);\n",
    "    x       = X(:,i);\n",
    "end\n",
    "\n",
    "function [a,x,p] = epsilongreedypolicy(s)\n",
    "    global vmodel epsilon;\n",
    "    aaa = actions(s)\n",
    "    X   = nextstates2onehots(s, aaa);\n",
    "    if rand() < epsilon\n",
    "        i     = randi([1 count(aaa)]);\n",
    "        p     = epsilon;\n",
    "    else\n",
    "        [~,Q] = forward(vmodel, X);\n",
    "        [~,i] = max(Q);\n",
    "        p     = 1-epsilon;\n",
    "    end\n",
    "    a = aaa(i);\n",
    "    x = X(:,i);\n",
    "end\n",
    "\n",
    "function [err,ok] = learnvalueapprox(X, Y)\n",
    "    global vmodel;\n",
    "    [vmodel,Z]     = forward(vmodel, X);\n",
    "    E              = cost(vmodel, Z, Y);\n",
    "    dE             = gradient(vmodel, Z, Y);\n",
    "    [vmodel,ggg,~] = backward(vmodel, Z, dE);\n",
    "    [vmodel,ggg]   = optimize_gradient(vmodel, ggg, 1);\n",
    "    new_vmodel     = update(vmodel, ggg);\n",
    "    err            = mean(E);\n",
    "    [~,Z]          = forward(new_vmodel, X);\n",
    "    E              = cost(new_vmodel, Z, Y);\n",
    "    delta          = mean(E) - err;\n",
    "    ok             = delta < 0;\n",
    "    if ok\n",
    "        vmodel = new_vmodel;\n",
    "    end\n",
    "end\n",
    "\n",
    "function s = start(agent)\n",
    "    global rivalpolicy;\n",
    "    s = game();\n",
    "    if agent == 2\n",
    "        a = rivalpolicy(s);\n",
    "        s = game(s, a); \n",
    "    end\n",
    "end\n",
    "\n",
    "function [s, reward, winner] = move(agent, s, a)\n",
    "    global rivalpolicy;\n",
    "    s = game(s, a);            # learning player moves\n",
    "    if iswin(s, a)             # learning player wins\n",
    "        reward = 1;\n",
    "        winner = agent;\n",
    "    elseif isover(s)           # draw\n",
    "        reward = 0;\n",
    "        winner = 0;\n",
    "    elseif player(s) == agent  # invalid move\n",
    "        error('invalid move');\n",
    "    else                       # continue\n",
    "        a = rivalpolicy(s);   \n",
    "        s = game(s, a);        # other player moves\n",
    "        if iswin(s, a)         # other player wins\n",
    "            reward = -1;\n",
    "            winner = player(s, a);\n",
    "        elseif isover(s)       # draw\n",
    "            reward = 0;\n",
    "            winner = 0;\n",
    "        else                   # continue\n",
    "            reward = 0;\n",
    "            winner = [];\n",
    "        end  \n",
    "    end  \n",
    "end\n",
    "\n",
    "function [winner,err,ok] = episode(agent)\n",
    "    global agentpolicy gamma;\n",
    "    winner = [];     # draw: 0, players: 1,2\n",
    "    X = zeros(18,9); # states as onehot vectors\n",
    "    Y = zeros(1,9);  # targets (rewards)\n",
    "    P = zeros(1,9);  # chosen action probs\n",
    "    n = 0;           # number of moves\n",
    "    s = start(agent);\n",
    "    do\n",
    "        [a,x,p]           = agentpolicy(s);\n",
    "        [s,reward,winner] = move(agent, s, a);\n",
    "        n                += 1;\n",
    "        X(:,n)            = x;\n",
    "        Y(1,n)            = reward;\n",
    "        P(1,n)            = p;\n",
    "    until !isempty(winner);\n",
    "    for i = n-1:-1:1\n",
    "        Y(1,i) += gamma * P(1,i+1) * Y(1,i+1);\n",
    "    end\n",
    "    [err,ok] = learnvalueapprox(X(:,1:n), Y(:,1:n));\n",
    "end\n",
    "\n",
    "function ratio = iteration(p, n, eps=1e-8)\n",
    "    global vmodel;\n",
    "    ERROR = zeros(1,n);\n",
    "    RATIO = zeros(1,n);\n",
    "    OK    = 0; \n",
    "    wins  = [0 0];\n",
    "    for t = 1:n\n",
    "        [winner,err,ok] = episode(p);\n",
    "        if winner > 0 \n",
    "            wins(winner) += 1; \n",
    "        end\n",
    "        if nnz(wins) == 2\n",
    "            ratio = wins(p) / wins(other(p));\n",
    "        else\n",
    "            ratio = 0;\n",
    "        end\n",
    "        ERROR(t) = err;\n",
    "        RATIO(t) = ratio;\n",
    "        OK      += 1*ok;\n",
    "        if t > 1 \n",
    "            edir = sign(err   - ERROR(t-1));\n",
    "            rdir = sign(ratio - RATIO(t-1));\n",
    "        else\n",
    "            edir = 0;\n",
    "            rdir = 0;\n",
    "        end\n",
    "        gradnorm = getunit(vmodel.optimizers,'gradient_clipping').norm;\n",
    "        updratio = getunit(vmodel.optimizers,'stats').ratio;        \n",
    "        showlog(1, 100, 'player %d, episode %d of %d (ok %d), gradnorm %f, updratio %f, wins %s, error %f %s, ratio %f %s', \n",
    "                         p, t, n, OK, gradnorm, updratio, mat2str(wins), err, dir2arrow(edir), ratio, dir2arrow(rdir));\n",
    "    end\n",
    "    figure('Position', [0 0 1000 400]);\n",
    "    hold on;\n",
    "    plot(1:n, ERROR, 'r');\n",
    "    plot(1:n, RATIO, 'g');\n",
    "    legend('objective', 'win ratio')\n",
    "    title('training history');\n",
    "end\n",
    "\n",
    "tau         = 5;\n",
    "gamma       = 1;\n",
    "epsilon     = 0.1;\n",
    "rivalpolicy = @randompolicy;\n",
    "agentpolicy = @taustochasticpolicy;\n",
    "vmodel      = model(180, {'dense', 300}, 'relu', {'dense', 100}, 'relu', {'dense', 1});\n",
    "vmodel      = optimization(vmodel, {'adam', 0.01}, {'gradient_clipping', 0.9}, 'stats');\n",
    "vmodel      = objective(vmodel, 'mse');\n",
    "\n",
    "printvar('vmodel.num_p');\n",
    "% printmodel('vmodel');\n",
    "\n",
    "play(100, @randompolicy);\n",
    "play(100, @randompolicy, @stochasticpolicy);\n",
    "play(100, @randompolicy, @deterministicpolicy);\n",
    "\n",
    "printstart();\n",
    "ratio = iteration(2, 1000);\n",
    "printend();\n",
    "\n",
    "play(100, @randompolicy, @stochasticpolicy);\n",
    "play(100, @randompolicy, @deterministicpolicy);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 5;\n",
    "\n",
    "printstart();\n",
    "ratio = iteration(2, 10000);\n",
    "printend(ipynb);\n",
    "\n",
    "play(100, @randompolicy, @stochasticpolicy);\n",
    "play(100, @randompolicy, @deterministicpolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 2;\n",
    "\n",
    "printstart();\n",
    "ratio = iteration(2, 10000);\n",
    "printend(ipynb);\n",
    "\n",
    "play(100, @randompolicy, @stochasticpolicy);\n",
    "play(100, @randompolicy, @deterministicpolicy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmodel = TUNE(vmodel, 'adam', 0.001);\n",
    "save('-binary',tmp('turns.mat'),'turns');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global turns;\n",
    "\n",
    "turns = struct();\n",
    "\n",
    "function k = state2key(s)\n",
    "    k = char(s(:)'+48);\n",
    "end\n",
    "\n",
    "function enum(s = game())\n",
    "    global turns;\n",
    "    key = state2key(s);\n",
    "    showlog(1, 50, '%d', numfields(turns));\n",
    "    if !isfield(turns,key) \n",
    "        turns.(key) = player(s);\n",
    "        for a = actions(s)\n",
    "            [s_,winner] = game(s,a);\n",
    "            if isempty(winner)\n",
    "                enum(s_);\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "printstart();\n",
    "enum();\n",
    "printend(sprintf('total states %d', numfields(turns)));    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "4.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
